# Problem 1

In this problem, we apply Principal Component Analysis to stock market index data.  We will use records of daily closing prices of S&P 500 stocks from January 1, 2011 to December 31, 2014 retrieved through Yahoo Finance. The data is stored in the attached file named “SP500_close_price_no_missing.csv”.  There are actually only 471 stocks in this file, the rest of the stocks were not included either because Yahoo Finance returned an error or because the stock was not listed as of January 1, 2011.  The file named “SP500_ticker.csv” contains ticker information for each included stock, as well as the corresponding company name and its industry sector assignment.  Note: I also included the raw data before missing values are imputed, for those who would like to challenge yourself, you may try to do missing value imputation yourself.
pip install numpy scikit-learn
pip install numpy pandas matplotlib scikit-learn
#### a) Fit a PCA model to log returns  (log return = log( Price [t+1]/Price [t]) derived from stock price data and complete the following tasks
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

# Load the CSV file
data = "SP500_close_price_no_missing.csv"
df = pd.read_csv(data, index_col=0)

# Display the first few rows of the data
df.head(3)
**1. Derive log returns from the raw stock price dataset**
import pandas as pd
import numpy as np

# Read the dataset
df = pd.read_csv("SP500_close_price_no_missing.csv")

# Convert the 'date' column to datetime type
df['date'] = pd.to_datetime(df['date'])

# Calculate log returns
log_returns = np.log(df.set_index('date') / df.set_index('date').shift(1))

# Drop the first row since it will have NaN for log returns
log_returns = log_returns.dropna()

# Display the first few rows of the data with log returns
print("\nSample of the data with log returns:")
log_returns.head()
# Standardize the log returns
scaler = StandardScaler()
log_returns_standardized = scaler.fit_transform(log_returns)

# Fit PCA model
pca = PCA()
pca.fit(log_returns_standardized)

# Access the principal components and explained variance
principal_components = pca.components_
explained_variance = pca.explained_variance_ratio_

# Print results
print("\nPrincipal Components:")
print(principal_components)
print("\nExplained Variance:")
print(explained_variance)

# Plot the cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance)
plt.plot(cumulative_explained_variance, marker='o')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance of Principal Components')
plt.grid(True)
plt.show()
**2. Plot a scree plot which shows the distribution of variance contained in subsequent principal components sorted by their eigenvalues.**
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Perform PCA on log returns data
pca = PCA()
pca.fit(log_returns.dropna())

# Scree plot
eigenvalues = pca.explained_variance_
plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Eigenvalue')
plt.show()
**3. Create a second plot showing cumulative variance retained if top N components are kept after dimensionality reduction (i.e. the horizontal axis will show the number of components kept, the vertical axis will show the cumulative percentage of variance retained).**
# Get the explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_

# Plot the scree plot and cumulative explained variance
fig, ax1 = plt.subplots()

# Scree plot
ax1.plot(np.arange(1, len(explained_variance_ratio) + 1), pca.explained_variance_, 'o-', label='Eigenvalues', color='b')
ax1.set_xlabel('Principal Components')
ax1.set_ylabel('Eigenvalues', color='b')
ax1.tick_params('y', colors='b')
ax1.set_title('Scree Plot and Explained Variance Ratio')

# Cumulative explained variance plot
ax2 = ax1.twinx()
cumulative_variance_retained = np.cumsum(explained_variance_ratio)
ax2.plot(np.arange(1, len(cumulative_variance_retained) + 1), cumulative_variance_retained,
         's-', label='Cumulative Explained Variance', color='r')
ax2.set_ylabel('Cumulative Explained Variance', color='r')
ax2.tick_params('y', colors='r')

# Show the plot
plt.show()
**4. How many principal components must be retained in order to capture at least 80% of the total variance in data?**
# Calculate cumulative variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Plot cumulative variance
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.title('Cumulative Variance Plot')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Variance (%)')
plt.show()

# Determine the number of principal components needed for 80% variance
n_components_80 = np.argmax(cumulative_variance >= 0.8) + 1
print(f"Number of components needed for 80% variance: {n_components_80}")
### b) Analysis of principal components and weights
**1. Compute and plot the time series of the 1st principal component and observe temporal patterns. Identify the date with the lowest value for this component and conduct a quick research on the Internet to see if you can identify event(s) that might explain the observed behavior.**
pca.fit(log_returns_standardized)

# Compute and plot the time series of the 1st principal component
principal_components = pca.transform(log_returns_standardized)
pc1 = principal_components[:, 0]

# Plot time series of the 1st principal component with dates on the x-axis
plt.figure(figsize=(10, 5))
plt.plot(df['date'][1:], pc1, label='PC1')  # Use df['date'][1:] to exclude the first date
plt.title('Time Series of 1st Principal Component')
plt.xlabel('Date')
plt.ylabel('Principal Component Value')
plt.legend()
plt.show()

# Identify the date with the lowest value for the 1st principal component
min_pc1_index = np.argmin(pc1)
min_pc1_date = df['date'][min_pc1_index + 1]  # Adding 1 to get the correct date
print(f"Date with the lowest value for the 1st principal component: {min_pc1_date}")
**2. Extract the weights from the PCA model for 1st and 2nd principal components.**
# Extract weights from the PCA model for the 1st and 2nd principal components
weights_pc1 = pca.components_[0]
weights_pc2 = pca.components_[1]

# Print the weights
print("Weights for the 1st principal component:")
print(weights_pc1)
print("\nWeights for the 2nd principal component:")
print(weights_pc2)
**3. Create a plot to show weights of the 1st principal component grouped by the industry sector (for example, you may draw a bar plot of mean weight per sector). Observe the distribution of weights (magnitudes, signs). Based on your observation, what kind of information do you think the 1st principal component might have captured?**
import seaborn as sns

# Assuming df contains the stock price data
data = "SP500_close_price_no_missing.csv"
df = pd.read_csv(data, index_col=0)
ticker_data = pd.read_csv('SP500_ticker.csv', encoding='ISO-8859-1')

# Create a DataFrame with tickers and weights of the 1st principal component
weights_df = pd.DataFrame({'ticker': df.columns, 'Weight_PC1': weights_pc1})

# Merge with ticker_data to get sector information
weights_df = pd.merge(weights_df, ticker_data[['ticker', 'sector']], on='ticker')

# Remove leading and trailing whitespaces and convert sector names to lowercase
weights_df['sector'] = weights_df['sector'].str.lower().str.strip()

# Define a custom color palette
custom_palette = "deep"

# Plot weights for the 1st principal component grouped by industry sector
plt.figure(figsize=(12, 6))
sns.barplot(x='sector', y='Weight_PC1', data=weights_df, palette=custom_palette)
plt.title('Weights of 1st Principal Component by Industry Sector')
plt.xlabel('Industry Sector')
plt.ylabel('Weight')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

plt.show()
**4. Make a similar plot for the 2nd principal component.  What kind of information do you think does this component reveal? (Hint: look at the signs and magnitudes.)**
# Create a DataFrame with tickers and weights of the 2nd principal component
weights_df_pc2 = pd.DataFrame({'ticker': df.columns, 'Weight_PC2': weights_pc2})

# Merge with ticker_data to get sector information
weights_df_pc2 = pd.merge(weights_df_pc2, ticker_data[['ticker', 'sector']], on='ticker')

# Remove leading and trailing whitespaces and convert sector names to lowercase
weights_df_pc2['sector'] = weights_df_pc2['sector'].str.lower().str.strip()

# Plot weights for the 2nd principal component grouped by industry sector
plt.figure(figsize=(12, 6))
sns.barplot(x='sector', y='Weight_PC2', data=weights_df_pc2, palette=custom_palette)
plt.title('Weights of 2nd Principal Component by Industry Sector')
plt.xlabel('Industry Sector')
plt.ylabel('Weight')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

plt.show()
**5. Suppose we wanted to construct a new stock index using one principal component to track the overall market tendencies. Which of the two components would you prefer to use for this purpose, the 1st or the 2nd? Why?**
### c) Bonus points (20 points)
Re-run the PCA analysis with the most updated data downloaded from the available stock price API.

pip install alpha_vantage
pip install adjustText
import requests
import pandas as pd

# Replace the apikey below with your own key from https://www.alphavantage.co/support/#api-key
url = 'https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY_ADJUSTED&symbol=IBM&apikey=VCK0T7NL7CSZV89Q'
r = requests.get(url)
data = r.json()

# Convert JSON data to a DataFrame
df_list = []

for date, values in data['Monthly Adjusted Time Series'].items():
    row = {
        'Date': date,
        'Open': values['1. open'],
        'High': values['2. high'],
        'Low': values['3. low'],
        'Close': values['4. close'],
        'Adjusted Close': values['5. adjusted close'],
        'Volume': values['6. volume'],
        'Dividend Amount': values['7. dividend amount']
    }
    df_list.append(row)

IBM_stock = pd.DataFrame(df_list)

# Print the DataFrame
IBM_stock.head()
## `Log Returns`
# Assuming 'Date' is already in datetime format
IBM_stock['Date'] = pd.to_datetime(IBM_stock['Date'])

# Convert numeric columns to appropriate data types
numeric_columns = ['Open', 'High', 'Low', 'Close', 'Adjusted Close', 'Volume', 'Dividend Amount']
IBM_stock[numeric_columns] = IBM_stock[numeric_columns].apply(pd.to_numeric, errors='coerce')

# Calculate log returns
IBM_stock['Log_Returns'] = np.log(IBM_stock['Close'] / IBM_stock['Close'].shift(1))

# Drop the first row since it will have NaN for log returns
IBM_stock = IBM_stock.dropna()

# Display the DataFrame with log returns
print(IBM_stock[['Date', 'Close', 'Log_Returns']])





```
- Explained Variance Ratios
- Principal Components
- Scree Plot
- Cumulative Explained Variance of Principal Component
```




import numpy as np
from sklearn.decomposition import PCA

# Drop rows with missing values
IBM_stock = IBM_stock.dropna()

# Calculate log returns with a small constant to avoid division by zero
log_returns = np.log(IBM_stock[numeric_columns] / IBM_stock[numeric_columns].shift(1) + 1e-8)

# Drop the first row since it will have NaN for log returns
log_returns = log_returns.dropna()

# Manually standardize the log returns and replace infinite or too large values
log_returns_standardized = (log_returns - log_returns.mean()) / log_returns.std()
log_returns_standardized.replace([np.inf, -np.inf], np.nan, inplace=True)
log_returns_standardized.fillna(0, inplace=True)

# Fit PCA model
pca = PCA()
pca.fit(log_returns_standardized)

# Print the explained variance ratios
explained_variance_ratio = pca.explained_variance_ratio_
print("Explained Variance Ratios:")
print(explained_variance_ratio)

# Display the principal components and their weights
principal_components_df = pd.DataFrame(principal_components, columns=[f'PC_{i+1}' for i in range(principal_components.shape[1])])
print('Principal Components:')
print(principal_components_df.head())

# Plot the scree plot
eigenvalues = pca.explained_variance_
plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, marker='o')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Eigenvalue')
plt.show()

# Create a second plot showing cumulative explained variance
cumulative_explained_variance = np.cumsum(explained_variance_ratio)
plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance')
plt.title('Cumulative Explained Variance of Principal Components')
plt.grid(True)
plt.show()

# Calculate the number of components needed for 80% variance
n_components_80 = np.argmax(cumulative_explained_variance >= 0.8) + 1
print(f"Number of components needed for 80% variance: {n_components_80}")
## `Time series of the 1st principal component`
import matplotlib.pyplot as plt

# You might need to adjust this based on your data
n_features = log_returns_standardized.shape[1]

# Fit PCA model
pca = PCA(n_components=n_features)
pca_result = pca.fit_transform(log_returns_standardized)

# Extract the 1st principal component
pc1_time_series = pca_result[:, 0]

# Assuming your DataFrame has a 'Date' column
# Convert 'Date' to datetime format if it's not already
IBM_stock['Date'] = pd.to_datetime(IBM_stock['Date'])

# Create a DataFrame for the time series
pc1_df = pd.DataFrame({'Date': IBM_stock['Date'].iloc[:len(pc1_time_series)], 'PC1': pc1_time_series})

# Plot the time series of the 1st principal component
plt.figure(figsize=(12, 6))
plt.plot(pc1_df['Date'], pc1_df['PC1'], label='1st Principal Component')
plt.title('Time Series of the 1st Principal Component')
plt.xlabel('Date')
plt.ylabel('Principal Component Value')
plt.legend()
plt.show()


```
- Explained variance ratio
- Principal components and their weights

```


# Extract relevant numerical columns for PCA
numeric_columns = IBM_stock.drop('Date', axis=1).select_dtypes(include='number')

# Standardize the data
scaler = StandardScaler()
scaled_data = scaler.fit_transform(numeric_columns)

# Perform PCA
pca = PCA()
pca_result = pca.fit_transform(scaled_data)

# Explained variance ratio
explained_variance_ratio = pca.explained_variance_ratio_

# Plot explained variance
plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o')
plt.title('Explained Variance Ratio by Principal Component')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.show()

# Display the principal components and their weights
principal_components = pd.DataFrame(pca.components_, columns=numeric_columns.columns)
print('Principal Components and Their Weights:')
print(principal_components)

# ```Biplot (2D visualization) ```


from adjustText import adjust_text

# Optional: Biplot (2D visualization)
plt.figure(figsize=(8, 8))
texts = []

for i, feature in enumerate(numeric_columns.columns):
    x_pos = pca.components_[0, i]
    y_pos = pca.components_[1, i]

    plt.arrow(0, 0, x_pos, y_pos, color='r', alpha=0.5)
    texts.append(plt.text(x_pos, y_pos, feature, color='b', ha='center', va='center'))

# Adjust text labels to avoid overlap
adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'))

plt.title('Biplot: Principal Components and Feature Vectors')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
# Problem 2
Let us experiment with a few feature selection methods. We will use data stored in ‘BMI.csv’ file. This data contains measurements of Body Mass Index (BMI) obtained for a number of human subjects. The goal is to predict fat percentage (fatpctg) using all other features available in data.
**a) Wrapper method: Search for the best set of features using backward and forward stepwise regression**
import pandas as pd
import statsmodels.api as sm
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Load the CSV file
data = pd.read_csv("BMI.csv")

# Define the target variable and features
target_variable = 'fatpctg'
features = [col for col in data.columns if col != target_variable]

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

def backward_stepwise_regression(data, features, target_variable):
    remaining_features = set(features)
    selected_features = []
    current_score, best_new_score = float('inf'), float('inf')

    while remaining_features and current_score == best_new_score:
        scores_with_candidates = []

        for candidate in remaining_features:
            model = sm.OLS(data[target_variable], sm.add_constant(data[selected_features + [candidate]])).fit()
            score = model.aic
            scores_with_candidates.append((score, candidate))

        scores_with_candidates.sort(reverse=True)
        best_new_score, best_candidate = scores_with_candidates.pop()

        if current_score > best_new_score:
            remaining_features.remove(best_candidate)
            selected_features.append(best_candidate)
            current_score = best_new_score

    return selected_features

def forward_stepwise_regression(data, features, target_variable):
    remaining_features = set(features)
    selected_features = []
    current_score, best_new_score = float('inf'), float('inf')

    while remaining_features and current_score == best_new_score:
        scores_with_candidates = []

        for candidate in remaining_features:
            model = sm.OLS(data[target_variable], sm.add_constant(data[selected_features + [candidate]])).fit()
            score = model.aic
            scores_with_candidates.append((score, candidate))

        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates.pop(0)

        if current_score > best_new_score:
            remaining_features.remove(best_candidate)
            selected_features.append(best_candidate)
            current_score = best_new_score

    return selected_features

# Backward stepwise regression
selected_features_backward = backward_stepwise_regression(train_data, features, target_variable)
print("Selected features (Backward Stepwise Regression):", selected_features_backward)

# Forward stepwise regression
selected_features_forward = forward_stepwise_regression(train_data, features, target_variable)
print("Selected features (Forward Stepwise Regression):", selected_features_forward)

**b) Filter method:  output a ranking of features using correlation statistics (i.e. between any of the input variables and output)**
# Define the target variable
target_variable = 'fatpctg'

# Calculate the correlation between each input variable and the target variable
correlation_ranking = data.corr()[target_variable].abs().sort_values(ascending=False)

# Display the ranking
print("Feature Ranking Based on Correlation with", target_variable)
print(correlation_ranking)
**c) Embedded method: (1) Lasso regression; (2) random forest (feature importance ranking)**
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler

# Define the target variable and features
target_variable = 'fatpctg'
features = [col for col in data.columns if col != target_variable]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data[features], data[target_variable], test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Fit Lasso regression model
lasso_model = Lasso(alpha=0.1)  # You can adjust the alpha parameter
lasso_model.fit(X_train_scaled, y_train)

# Get the coefficients and corresponding feature names
lasso_coefficients = pd.Series(lasso_model.coef_, index=features)

# Display the feature importance ranking from Lasso regression
lasso_ranking = lasso_coefficients.abs().sort_values(ascending=False)
print("Feature Ranking Based on Lasso Regression Coefficients:")
print(lasso_ranking)
from sklearn.ensemble import RandomForestRegressor

# Fit Random Forest model
random_forest_model = RandomForestRegressor(n_estimators=100, random_state=42)  # You can adjust the number of estimators
random_forest_model.fit(X_train, y_train)

# Get the feature importances
feature_importances = pd.Series(random_forest_model.feature_importances_, index=features)

# Display the feature importance ranking from Random Forest
rf_ranking = feature_importances.sort_values(ascending=False)
print("\nFeature Ranking Based on Random Forest Feature Importances:")
print(rf_ranking)
